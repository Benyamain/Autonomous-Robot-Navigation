Paper Information
Authors: Danilo Giacomin Schneider and Marcelo Ricardo Stemmer [1, 2]
Title: CNN-based Multi-Object Detection and Segmentation in 3D LiDAR Data for Dynamic Industrial Environments [1, 2]
Type: Preprint [1]
Publication Date: Posted 7 October 2024 [1]
Number of Citations: The provided source does not contain the number of citations.
Research Focus
Main Purpose: The main objective is to design and evaluate a CNN that enables multi-source data aggregation and the generation of a semantic 2D map for wheeled mobile robots to improve path-planning and navigation [3]. The paper addresses the challenge of autonomous navigation in dynamic environments for mobile robotic systems by using CNNs for multi-object detection in 3D space and 2D segmentation using Bird’s Eye View (BEV) maps derived from 3D LiDAR data [4].
Key Questions Addressed:How can advanced deep learning techniques, synthetic data generation, and ROS2-based communication frameworks be leveraged to develop a robust perception system for autonomous navigation in dynamic environments, specifically focusing on multi-object detection and segmentation using 3D LiDAR data [3]?
How to perform accurate and fast detection, tracking, and building of a semantic 2D map of the environment on top of a static map using the Birds-Eye-View (BEV) representation of 3D LiDAR data [3]?
Key Contributions:A CNN-based detection and segmentation model integrated into a ROS2-based framework, facilitating communication between mobile robots and a centralized node for data aggregation and map creation [4].
A synthetic dataset based on a simulation environment to train and evaluate the model, addressing the scarcity of labeled real-world datasets [4]. The dataset includes 3D object detection and semantic segmentation annotations in BEV 3D LiDAR data [5].
A centralized algorithm that runs at 10Hz and synchronizes detections and segmentations from multiple sources, maintaining a dictionary table of instances with characteristics such as class, 2D pose, ray, and occupancy mask [6].
Technical Details
Machine Learning Methods Used:Convolutional Neural Networks (CNNs) are used for multi-object detection and segmentation due to their ability to learn from vast amounts of data and increase performance over time [4, 7]. The CNN architecture uses a ResNet-50 backbone and KeyPoint Feature Pyramid Network (FPN) for 3D object detection, and a Fully Convolutional Network (FCN) for semantic segmentation [8].
Birds-Eye-View (BEV) representation of 3D LiDAR data is used for accurate and fast detection, tracking, and building a semantic 2D map [3, 4]. A 10x10 meter area in front of the sensor is used to build a 608x608 pixels BEV representation [9].
Datasets Used:Synthetic Dataset: A dataset generated using the Gazebo Simulator, containing 10593 annotated LiDAR samples divided into training (75.52%), validation (5.93%), and test (18.9%) sets [4, 5]. The classes consist of person, 6 different mobile robots, and 7 different movable objects [5].
NVIDIA r2b dataset: A subset of the NVIDIA r2b dataset is used for evaluation in the real world [4]. Two sequences from this dataset, r2b_storage and r2b_hallway, which contain humans walking in front of a mobile robot, were annotated and used to evaluate the model on the person class [10].
The original dataset is available on GoogleDrive [11]. The model weights and utilities are also available on GoogleDrive [11].
Implemented Algorithms and Tools:ROS2: The Robot Operating System 2 is used as a framework to facilitate communication between mobile robots and a centralized node for data aggregation and map creation [4, 5].
Gazebo Simulator: Used as the simulation platform for generating the synthetic dataset [5].
PyTorch: Used for CNN design, training, and inference [9].
Evaluation Metrics:Per-pixel precision, recall, and Intersection over Union (IoU) are used for evaluating semantic segmentation and occupancy mapping [12].
Average Precision (AP) is used for object detection for each class, and Mean Average Precision (mAP) for class-wide evaluation [12].
Absolute errors in positions "x", "y", and angle "yaw" are calculated to every instance in the final ID dictionary and then averaged to estimate the performance of the final system localization of the movable objects [13].
Outcomes
Main Findings and Conclusions:The proposed approach shows promising performance and potential applicability in future assembly systems [4]. The model can predict the class label of each pixel without many false positives or false negatives [14].
The model can partially predict and segment the person class in real-world scenarios even with only synthetic training data [15].
Quantitative and qualitative results for the centralized node indicate fast and accurate data aggregation for tracking object locations and mapping their occupancy [16].
The results show good precision in the simulation domain [17].

3. In this paper, we investigate the following research question: How can we leverage advanced deep learning techniques, synthetic data generation, and ROS2-based communication frameworks to develop a robust perception system for autonomous navigation in dynamic environments, specifically focusing on multi-object detection and segmentation using 3D LiDAR data? Our hypothesis is that by using the Birds-Eye-View (BEV) representation of 3D LiDAR data, it is possible to perform accurate and fast detection, tracking and building of a semantic 2D map of the environment on top of the static map. The main objective is to design and evaluate a CNN that allows multi-source data aggregation and generation of this map for wheeled mobile robots path-planning and navigation.

4. Abstract: Autonomous navigation in dynamic environments presents a significant challenge for mobile robotic systems. In this paper, we propose a novel approach utilizing Convolutional Neural Networks (CNNs) for multi-object detection in 3D space and 2D segmentation using Bird’s Eye View (BEV) maps derived from 3D LiDAR data. Our method aims to enable mobile robots to localize movable objects and their occupancy, crucial for safe and efficient navigation. To address the scarcity of labeled real-world datasets, a synthetic dataset based on simulation environment is generated to train and evaluate our model. Additionally, we employ a subset of the NVIDIA r2b dataset for evaluation in real-world. Furthermore, we integrate our CNN-based detection and segmentation model into a ROS2-based framework, facilitating communication between mobile robots and a centralized node for data aggregation and map creation. Our experimental results demonstrate promising performance, showcasing the potential applicability of our approach in future assembly systems. While further validation with real-world data is warranted, our work contributes to the advancement of perception systems by proposing a solution for multi-source multi-object tracking and mapping.

5. 3. Materials and Methods
Tools: As Simulation platform, the Gazebo Simulator is used due to it’s open source development, integrated annotation sensors and facilitated communication with ROS2 framework [26,27].
For dataset generation, we extended the work done in [28] to include 3D object detection and semantic segmentation annotations in BEV 3D Lidar data. Using this automated dataset generation and 23 simulation models divided in 14 classes, we created the synthetic dataset containing a total of 10593 annotated LiDAR samples divided in training (75,52%), validation (5,93%) and test (18,9%) sets. The classes consist of person, 6 different mobile robots and 7 different movable objects, and we specified 0,5 meter for LiDAR height position, considering that it would be attached to one of the robots.

6. Another consideration is that the detection robots have a self global localization algorithm running, and they publish their poses together with the detections from the CNN. This way, local detection poses can be transformed in global poses and detection robots can also be integrated in the id table.
In summary, the centralized algorithm runs at 10hz and at each iteration it synchronizes the received detections and segmentation from all sources and maintain a dictionary table where each line represents an instance with an id number and the columns contain a characteristic of the particular instance, such as class, 2D pose, ray, and occupancy mask. First it iterates over each source, then it iterates over the dictionary for a match with the detection robot (source) and transforms every detection from that robot to global poses. For each transformed detection in iterates over the dictionary for a match and, when necessary, deals with miss class, twists yaw angle, updates the dictionary and add a new instance when no match is found. After every source iteration, it checks for doubled detections in the dictionary and builds the final map on top of the empty environment map. The long-term time complexity of the centralized algorithm is O(r ∗m ∗ (d + 1) + m). Where r is the number of detection sources, m the size of the id dictionary and d the number of detections. The pseudo algorithm of the python script is given in Algorithm A1 on Appendix A.

7. contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions, or products referred to in the content.
2 of 15
both for navigation and filtering the sensor data, resulting in more precise global localization or more effective path planning.
Regarding the tasks of tracking and segmenting movable objects, including robots and humans, Convolutional Neural Networks (CNNs) have been highly used due to their ability of learning from vasts amount of data and increasing performance over time. Training CNNs requires an extensive and varied dataset of precise annotations, a process often laborious and costly when done manually. To mitigate this challenge, simulation-based data generation has surfaced as a prospective solution. This approach leverages synthetic data to train CNNs for tasks such as object detection and segmentation [8,9].

8. In the architecture design, shown in Figure 1, we chose the ResNet-50 backbone and KeyPoint Feature Pyramid Network (FPN) for 3D object detection, and Fully Convolutional Network (FCN) for semantic segmentation.
ResNet-50 [33] offers a good balance between depth and computational cost, making it suitable for real-time applications. Its skip connections alleviate the vanishing gradient problem, enabling effective training of deep networks. While newer architectures exist, ResNet-50 remains widely adopted, with extensive pre-trained models and strong empirical evidence of its effectiveness across various tasks.

9. For CNN design, training and inference, the PyTorch library is used in this project. Full dataset link, ROS2 packages, jupyter notebooks and video results are maintained in [29].
CNN design and training: BEV representations are usually encoded using height, intensity and density channels [30]. On our specific use case, despite of LiDAR’s distance sensing range and 360 degrees horizontal field of view, we defined an area of 10x10 meters in front of the sensor to build a 608 × 608 pixels BEV representation. The reason is the consideration of the sensors’ height position and its attachment to one side of the robot.

10. Total w/o background 80.24 82.06 68.79 mAP = 94.70
Figure 3. First and second frames predictions. RGB image with 3D bounding boxes visualization (a and b, respectively), BEV input with oriented bounding boxes (c and e, respectively), and semantic segmentation with oriented bounding boxes (d and f, respectively). Source: Authors.
The r2b dataset consists of sequences of collected sensor data stored in rosbags [38]. The sequences have data from a 3D LiDAR similar to the one we simulated, and two of these sequences contain humans walking in front of a mobile robot: r2b_storage and r2b_hallway. Due to the unavailability of a real-world dataset for object detection and segmentation on LiDAR BEV maps with the same simulated movable objects, each frame of the two sequences were annotated and used to evaluate our model on the person class. Results are given in Table 2, with semantic segmentation and oriented bounding boxes evaluation. Varying the IoU threshold from 10% to 90% with 10% pace, oriented object detection resulted in mAP@0.1:0.1:0.9 = 38, 84%, and with the variation from 50% to 95% with 5% pace

11. Institutional Review Board Statement: Not applicable.
Informed Consent Statement: Not applicable.
Data Availability Statement: The original dataset presented in the study is openly available in GoogleDrive at https://drive.google.com/drive/folders/1HEUbte6S7996iOndTYddwkNz7qlJFa4Y?usp=sharing. The model weights and utilities obtained after training and used in the ROS2 packages are openly available in GoogleDrive at https://drive.google.com/drive/folders/1sE40jEmxJZB0Am2HAZoG8_yxZd534em1?usp= sharing. Restrictions apply to the availability of r2b dataset. Data were obtained from NVIDIA and are available at https://catalog.ngc.nvidia.com/orgs/nvidia/teams/isaac/resources/r2bdataset2023 in Creative Commons - Attribution 4.0 International license with ethical considerations. Modifications of r2b dataset, as well as used scripts, ROS2 packages and jupyter notebooks for train, test and validation of the model are openly available in GitHub at https://github.com/danilogsch/Coop-SLAM.

12. Evaluation metrics: In assessing the performance of various tasks within my system, distinct evaluation metrics were employed to effectively gauge their efficacy. For semantic segmentation and occupancy mapping, the evaluation relied on per-pixel precision, recall, and Intersection over Union (IoU), averaged across all available samples.
Conversely, for object detection, Average Precision (AP) served as the primary metric for each class and Mean Average Precision (mAP) for classes-wide evaluation. Derived from the area under the precision-recall curve, with an IoU threshold typically set at 0.5 to determine true positives, false positives and false negatives. The area is calculated in this paper using trapezoidal area function interpolating all points for calculation. This metric aligns with established standards such as those observed in the Pascal VOC 2007 and DOTA datasets [36].

13. The absolute errors in positions "x", "y" and angle "yaw" are calculated to every instance in the final id dictionary, then averaged to give an estimation about the performance of final system localization of the movable objects.
By utilizing these evaluation metrics, a thorough assessment of system performance was achieved across diverse tasks, ensuring a comprehensive understanding of its capabilities and limitations.
4. Results
The neural network training occurred for more than 300 epochs with cosine learning rate decay [37], starting with a learning rate of 0,001. Different batch sizes were used, more specifically a batch size 4 for an onboard NVIDIA RTX3070 and 10 for a V100. For data augmentation, each sampĺe had a probability of horizontal flip, random translation on LiDAR’s "x" axis and random rotation on LiDAR’s "z" axis. As the automated dataset generation tool is available, the dataset was also increased during training with more samples of classes with low evaluation scores. The weights obtained in the epoch with the lowest total loss on valuation set were selected for the following experiments.

14. Table 4. Average timing values in seconds for each part of the pipeline.
Pointcloud2_msg to Array Points Filtering BEV Building Prediction Data Aggregation
0.1485 0.0009 0.0090 0.0406 0.0015
5. Discussion
The overall scores of precision, recall and IoU metrics shown in Table 1 indicates that the model can predict the class label of each pixel without many false positives nor false negatives, the IoU indicates that it predicts more than 45% of all classes’ pixels, fairly indicating if each pixel is occupied or not by one determined class. The person class is the one with lower IoU score among the classes, probably due to the predictions of arms and legs during walking animations and frequent occlusions in the test set.

15. The quantitative results on real-world data, shown in Table 2, indicates that even with only synthetic training data the model can partially predict and segment the person class in the real-world scenarios. One reason for the decrease on each metric score is the difference between simulation and real scenario domains, where the walking action is more natural than in the simulated animation. Also, in the first sequence the person is carrying a box and in the training set we only used generic walking animations. Other reason is the high difficulty level on manually annotating the ground truth, since only the robots’ perspective of one side of the humans is available, it is hard to determine and annotate both the direction and the segmentation of them in the 2D BEV space. These reasons can be visualized on Figure 7, and they reinforce the difficulty of creating a real-world data set for this multi-task neural network, at least two sensors on opposite sides of the target object would be needed to annotate the segmentation task and a gyroscope to determine the direction of objects that do not have internal sensors.

16. Figure 7. Synthetic dataset BEV sample and its automatically generated segmentation ground truth (a and b, respectively). BEV sample of r2b dataset (c), segmentation obtained with the polygon (contour in red) though manually clicked vertices, blind sides were annotated in a non-convex manner. Source: Authors.
For the centralized node, both the quantitative and qualitative results indicate a fast and accurate data aggregation on the tasks of tracking the objects locations and mapping their occupancy. The yaw predictions are post-processed for dealing with symmetric classes and random 180 degrees flips from frame-to-frame. On the second scenario, one of the models leaves the field of view of all three detection robots, and comes back after a few seconds being re-identified with same ID as before in the dictionary.

17. 6. Conclusions
In conclusion, we presented a multi-task CNN for 3D detection and semantic segmentation in BEV representations of 3D LiDAR data, and a centralized ROS2 node for object tracking and semantic map building. The results show good precision in the simulation domain. Despite the impossibility of benchmark the proposed CNN with state-of-the-art single task CNNs, the synthetic dataset generated
12 of 15
alongside the tool used for creating it are public available for future researches and benchmarks. The results on real-world domain are not as good as in simulation, but they are promising.
