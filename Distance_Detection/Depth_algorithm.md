
Segmentation for depth cameras
The ultimate goal of this section is to provide scene distance information.

(1) Calculate the vertical declination of the pixels from each point to the center optical axis and translate it to the horizontal z-axis.
In order to get better results for object recognition, our RGB and depth cameras have a pitch angle of 0.2 rad, which is used to avoid not being able to see the top surface of higher objects (for CNN training) caused by too low viewing angle. So if we want to characterize the actual position of the object using the data from the depth camera, we need to overcome the vertical distortion. Translate all depth quantities to the horizontal z-axis. (The origin of the coordinate system is the upper left corner of the depth image, at which point the z-axis is the depth data. The horizontal z-axis assumes that the center optical axis is horizontal.) In actual calculations, we actually convert in rows of pixels.

(2) 8-neighborhood region growth based on mutation thresholding
We consider two neighboring pixels to be the same object when they are less than the mutation threshold. All labeled pixels are given after traversing all pixels. Then create multiple regions and initialize region seeds based on the marked pixels, traverse all neighboring pixels in the current region by breadth-first search (BFS), and use 8-neighborhood to detect all pixels that meet the threshold.

(3) Background Filtering
Based on the results of the previous step, I found that the largest regions in the results are earth and sky. This is not the distance we expect to recognize, so filtering the background is necessary. We actually tried a number of approaches, which I will detail. I used the geometric method at the beginning to exclude the ground by calculating the corresponding value of the ground in each pixel (in this case the unit is each pixel point, so we not only calculate the vertical declination but also give the horizontal declination). The problem was that the difference between the theoretical and actual values made it difficult to achieve the desired result even with the addition of a tolerance. Then we tried the 3D point cloud method with a single frame depth image, and we have to say that the 3D point cloud achieved very good results. However, considering that our final project requires real-time computation, the 3D point cloud will undoubtedly greatly increase the computational demand, which indicates that this is not a strategy that meets the conditions of our hardware. On the basis of the above, I considered that in depth images, the maximum value of each row of pixels is always the ground or the sky, and the actual detected object is always before the background. Based on this idea, I came up with the idea of using a difference algorithm for the maximum value. This difference algorithm not only requires very little computation, but also works very well: first, the maximum value of each row of depth pixels (the unit here is each row of pixels) is extracted to form an array. Since the depth of the ground background varies uniformly, in order to prevent a row of pixels from having no background at all, we form a difference array of the neighboring differences of the array of maximum values. Once the difference exceeds the threshold value, the corresponding value in the original maxima array is replaced by 0. This results in a very good representation of the processed segmentation depth image. Although a V-shaped splinter appears in all images, it is almost negligible after the mutation threshold is increased to 0.3.

(4) Horizontal projection
After the segmentation of the depth images is complete, I want to project these regions onto the ground to provide scene distance information to the decision maker. So first of all the depth image system is transformed to the robot system using 3D coordinate transformation. In this step: the depth system with the origin in the upper left corner of the depth image is transformed to the robot system on the ground. The computational procedure is not repeated. During the transformation I found that the xy-axis of the original depth system is in pixels, only the z-axis is in meters. But the robot coordinate system is in meters. In order to correct this problem we introduced the pinhole camera model and solved this unit problem by consulting the optical center coordinates and the horizontal and vertical focal lengths. Finally, in this projection form we tested two models: ray projection and parallel projection. After testing, it was found that the ray projection does not magnify the object equivalently to improve accuracy, but rather distorts the original object. So parallel projection was chosen for the final solution. The xy-plane projection of each pixel of the depth image in the robot coordinate frame is faithfully reflected.

The following image is the original image visualization of the depth image in npy format
![image](https://github.com/user-attachments/assets/e7d77ca8-9639-4dcd-a0d7-4b4f5ffa22f6)

The set of images shown below are the original RGB image, Segmentation of the depth image, Horizontal projection, Horizontal projection of the outer rectangle.
![Results_Segmentation](https://github.com/user-attachments/assets/2492af30-3258-42b5-9286-fa1006c2154a)

